{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Description: Legal Document Analysis and Summarization Tool using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk\n",
    "%pip install scikit-learn\n",
    "%pip install beautifulsoup4\n",
    "%pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def separate_numbers_and_text(text):\n",
    "    separated_text = re.sub(r\"(\\d+)([a-zA-Z]+)|([a-zA-Z]+)(\\d+)\", r\"\\1 \\2 \\3 \\4\", text)\n",
    "    return separated_text\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").text\n",
    "    text = text.lower()\n",
    "\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = \" \".join(word for word in text.split() if word not in stop_words)\n",
    "    text = separate_numbers_and_text(text)\n",
    "    text = \" \".join(text.split())\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # removing extra white space\n",
    "    text = contractions.fix(\n",
    "        text\n",
    "    )  # fixing contractions (ex: can't gets converted to cannot)\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+|https\\S+\", \"<URL>\", text)  # removing URLs\n",
    "    text = re.sub(r\"\\S+@\\S+\", \"<EMAIL>\", text)  # removing email IDs\n",
    "    text = re.sub(\n",
    "        r\"\\b(not)\\s+(\\w+)\\b\", r\"\\1_\\2\", text\n",
    "    )  # handling negations (ex: \"not good\" becomes \"not_good\")\n",
    "\n",
    "    text = \" \".join(stemmer.stem(word) for word in text.split())  # stemming\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def read_and_preprocess_data(folder_path):\n",
    "    judgement_path = os.path.join(folder_path, \"judgement\")\n",
    "    summary_path = os.path.join(folder_path, \"summary\")\n",
    "\n",
    "    judgement_files = sorted(os.listdir(judgement_path))\n",
    "    summary_files = sorted(os.listdir(summary_path))\n",
    "\n",
    "    judgements = []\n",
    "    summaries = []\n",
    "\n",
    "    for judgement_file, summary_file in zip(judgement_files, summary_files):\n",
    "        with open(\n",
    "            os.path.join(judgement_path, judgement_file), \"r\", encoding=\"utf-8\"\n",
    "        ) as jf:\n",
    "            judgement_text = jf.read()\n",
    "\n",
    "            judgement_text = preprocess_text(judgement_text)\n",
    "            judgements.append(judgement_text)\n",
    "\n",
    "        with open(\n",
    "            os.path.join(summary_path, summary_file), \"r\", encoding=\"utf-8\"\n",
    "        ) as sf:\n",
    "            summary_text = sf.read()\n",
    "\n",
    "            summary_text = preprocess_text(summary_text)\n",
    "            summaries.append(summary_text)\n",
    "\n",
    "    data = pd.DataFrame({\"Judgement\": judgements, \"Summary\": summaries})\n",
    "    return data\n",
    "\n",
    "\n",
    "# Paths to train and test folders\n",
    "train_folder_path = r\"D:\\Rohan\\ML\\Datasets\\legal_dataset\\IN-Abs\\train-data-small\"\n",
    "test_folder_path = r\"D:\\Rohan\\ML\\Datasets\\legal_dataset\\IN-Abs\\test-data\"\n",
    "\n",
    "# Reading and preprocessing train and test data\n",
    "train_data = read_and_preprocess_data(train_folder_path)\n",
    "test_data = read_and_preprocess_data(test_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'appeal lxvi 1949 appeal high court judicatur bombay refer section 66 indian incom tax act 1022 km munshi n p nathvani appel lant mc setalvad attorney gener india h j umrigar respond 1950 may 26 judgment court deliv mehr chand mahajan j appeal judgment high court judicatur bombay incom tax matter rais question whether munici pal properti tax urban immov properti tax payabl relev bombay act allow deduct section 9 1 iv indian incom tax act assesse compani invest compani deriv incom properti citi bombay assess year 1940 41 net incom assesse head properti comput incom tax offic sum rs 621764 deduct gross rent certain payment compani paid relev year rs 122675 municip properti tax rs 32760 urban properti tax deduct two sum claim provis section 9 act first item deduct sum rs 48572 allow ground item repres tenant burden paid assesse otherwis claim disal low appeal assesse appel sistant commission incom tax appel tribu nal unsuccess tribun howev agre refer two question law high court judicatur bombay name 1 whether municip tax paid applic compani allow deduct 555 provis section 9 1 iv indian incom tax act 2 whether urban immov properti tax paid applic compani allow deduct section 9 1 iv section 9 1 v indian incom tax act supplementari refer made cover third question rais us fore necessari refer high court answer three question neg henc appeal question determin whether munic ipal properti tax urban immov properti tax deduct allow claus iv sub section 1 section 9 act decis point depend firstli construct languag employ sub claus iv sub section 1 section 9 act secondli find true natur charact liabil owner relev bombay act payment tax section 9 along relev claus run thu 1 tax shall payabl assesse head incom properti respect bona fide annual valu properti consist build land appurten thereto owner subject follow allow name iv properti subject mortgag capit charg amount interest mortgag charg properti subject annual charg capit charg amount charg properti subject ground rent amount ground rent properti acquir construct repair renew recon struct borrow capit amount interest payabl capit seen claus iv consist four sub claus correspond four deduct allow 556 claus amend act 1939 claus iv contain first third fourth sub claus first sub claus interest deduct whether amount borrow secur properti spent properti question capit expenditur properti express capit charg sub claus cannot connot charg capit properti assess would redund open word clearli indic charg properti therefor opinion capit charg could mean charg creat capit sum ie charg secur discharg liabil capit natur 1933 privi council decid case bijoy singh dudhuria vs commission incom tax calcutta 1 assess section 9 assess ment gener incom assesse liabl pay mainten step mother charg asset decre court li abil voluntarili incur one cast law privi council held amount paid discharg liabil form part real incom includ assess though decis proceed principl outgo part assesse incom framer amend act 1939 want appar extend principl far assess properti concern even case obligatori payment made assesse incom properti charg payment second sub claus name properti subject annual charg capit charg amount charg ad sub claus appel invok support claim deduct municip urban properti tax present case view open word newli ad sub claus express capit charg also use therein cannot refer charg properti think must 1 ilr 60 cal 557 understood sens sub claus 1 say first sub claus provid deduc tion interest capit sum charg properti sub claus provid deduct annual sum charg sum capit sum limit word intend exclud case capit rais secur properti made repay instal commission incom tax bombay vs mahomedbhoy rowji 1 bench bombay high court consid mean word regard annual charg beau mont cj observ follow word think would cover charg secur annual liabil kania j said follow see charg annual unless mean charg respect payment made annual construct word follow judgment appeal gappum kanhaiya lal vs commission incom tax 2 connect appeal us bench allahabad high court agre construct place word bombay case ie word annual charg mean charg secur annual liabil therefor clear conflict judici deci sion mean phrase annual charg occur ring section 3 1 iv mean given natur mean word phrase capit charg beaumont cj case refer took view word mean charg capit kania j howev took differ view observ prepar accept sugg tion document provid certain payment made monthli annual charg immov properti estat individu becom capit charg allahabad judgment appeal 1 ilr 2 ilr 1944 558 word consid mean charg capit said annual charg mean charg secur discharg annual liabil capit charg mean charg secur discharg liabil capit natur think construct natu ral construct section right determin point whether tax disput fall within ambit phrase annual charg capit charg depend provis statut levi section 143 citi bombay municip act 1888 authoris levi gener tax build land citi primari respons pay properti tax lessor vide section 146 act order assess tax provis made determin annual rateabl valu build section 154 section 156 provid mainten assess book entri made everi offici year build citi rateabl valu name person primarili liabl payment properti tax build amount build assess section 167 lay assess ment book need prepar everi offici year public notic shall given accord section 160 162 everi year provis said sec tion section 163 167 shall applic year section lay procedur hear object complaint entri assess book provis clear liabil iti tax determin begin offici year tax annual one recur year year section 143 to 168 concern imposit liabil assess tax year amount tax year liabil payment determin act pre scribe collect chapter collect tax section 197 provid properti tax shall payabl 559 advanc half yearli instal first day april first day octob provis half yearli instal necessarili connot annual li abil word mean annual liabil discharg half yearli payment procedur also prescrib recoveri instal present bill notic demand distress sale final section 212 provid follow properti tax due act respect build land shall subject prior payment land revenu due provinci govern thereupon first charg upon said build ing land creat statutori charg build urban immov abl properti tax leviabl section 22 part vi bombay financ act 1932 on annual let valu properti duti collect tax laid municip manner case municip properti tax section 24 2 b term similar section 212 bombay municip act make land build secur payment tax also purpos section 9 indian incom tax act tax name munici pal properti tax well urban immov properti tax charact stand foot ing mr munshi learn counsel appel con tend tax assess annual valu land build annual tax although may collect interv six month sake conveni incom tax assess annual basi allow deduct payment made liabil incur previ ou year assess allow tax question fell clearli within languag section 9 1 iv learn attorney gener hand argu although tax assess year liabil pay aris begin 560 half year unless notic demand issu bill present liabil pay till charg section 212 act could possibl aris liabil pay half yearli advanc charg annual charg also suggest tax capit charg sens properti secur payment satisfi content rais learn attorney gener sound appar whole tenor two bombay act tax natur annual levi properti assess annual valu properti year annual liabil discharg half yearli instal liabil annual one properti subject provis claus iv sub sec tion 1 section 9 immedi attract great emphasi laid worddu use section 212 municip act said tax becom due act unless time payment arriv charg come exist till charg annual charg think correct construct section 212 word properti tax due act mean properti tax person liabl act tax payabl year made charg properti liabil charg co exist co exten sive provis act afford facil discharg liabil way affect true natur charact annual liabil discharg manner laid section 197 said properti cannot sold recoveri whole amount due year answer queri affirm ie proper ty liabl sale commission incom tax bombay vs mahomedbhoy rowji 1 beaumont cj reject claim deduct tax place relianc 1 ilr 561 section 9 1 v allow deduct respect sum paid account land revenu observ land revenu stand foot municip tax legislatur made special provis deduct sum payabl regard land revenu respect sum paid account municip tax circumst indic deduct allow purpos refer also made provi sion section 10 deal busi allow wherein deduct sum paid account land reve nue local rate municip tax allow conclud part judgment learn chief ju tice said necessari consid exact mean word suffi cient say cover municip tax made charg properti section 212 bombay municip act without determin exact mean word use statut seem us possibl arriv conclus tax within ambit claus elementari primari duti court give effect intent legislatur express word use outsid consider call aid find intent refer claus v section help land revenu charg paramount natur build land deduct respect amount mention express term municip tax hand stand foot land revenu law vari provinc provinc may necessarili charg properti case legi latur seem thought far municip tax properti concern fall within ambit claus iv deduct claimabl respect otherwis deduct allow section 10 head incom busi proceed differ foot construct section 9 aid section 10 apt mislead 562 kania j case arriv conclus influenc consider tax variabl charact ie liabl increas duce variou provis municip act charg natur conting charg great respect may point charg way may variabl conting na ture default made charg ever enforc whenev charg increas reduc year either payment addit borrow moss empir ltd vs inland revenu commission 1 held hous lord fact certain payment conting variabl amount affect charact annual payment word annual must taken qualiti recurr capabl recurr cunard truste vs inland revenu commission 2 held payment capabl recur rent therefor annual payment within mean schedul case iii rule 1 1 even though necessarili recurr year year fact vari amount immateri learn attorney gener view decis support view express kania j relianc place decis high court madra mamad keyi vs commission incom tax madra 3 money paid urban immov properti tax bombay financ act disallow inadmi sibl section 9 1 iv 9 1 v indian incom tax act decis mere follow view express commission incom tax bombay vs mahomedb hoy rowji 4 and arriv independ fresh reason much assist deci sion case allahabad high court 1 2 1948 1 aer 150 3 ilr 4 ilr 563 gappum kanhaiya lal vs commission incometax 1 connect appeal took correct view matter reason given therein approv result appeal allow two question refer high court incom tax tribun cite answer affirm appel cost appeal appeal allow'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"Judgement\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Judgement</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>appeal lxvi 1949 appeal high court judicatur b...</td>\n",
       "      <td>charg creat respect municip properti tax secti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>civil appeal no 94 1949 107 834 appeal judgmen...</td>\n",
       "      <td>agreement leas leas indian declar includ must ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxix 1950 applic articl 32 constitut india wri...</td>\n",
       "      <td>section 7 1 c east punjab public safeti act 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxxvii 1950 applic articl 32 constitut india w...</td>\n",
       "      <td>section 4 sub section 1 c east punjab public s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xvi 1950 appli cation articl 32 constitut writ...</td>\n",
       "      <td>held full court overrul preliminari object con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Judgement  \\\n",
       "0  appeal lxvi 1949 appeal high court judicatur b...   \n",
       "1  civil appeal no 94 1949 107 834 appeal judgmen...   \n",
       "2  xxix 1950 applic articl 32 constitut india wri...   \n",
       "3  xxxvii 1950 applic articl 32 constitut india w...   \n",
       "4  xvi 1950 appli cation articl 32 constitut writ...   \n",
       "\n",
       "                                             Summary  \n",
       "0  charg creat respect municip properti tax secti...  \n",
       "1  agreement leas leas indian declar includ must ...  \n",
       "2  section 7 1 c east punjab public safeti act 19...  \n",
       "3  section 4 sub section 1 c east punjab public s...  \n",
       "4  held full court overrul preliminari object con...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Judgement</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>appeal 101 1959 appeal special leav judgment o...</td>\n",
       "      <td>appel displac person west pakistan grant quasi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>appeal 52 1957 appeal judgment decre date apri...</td>\n",
       "      <td>appel respond owner adjoin collieri suit prese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>appeal no 45 46 1959 appeal special leav judgm...</td>\n",
       "      <td>respond firm claim exempt sale tax articl 286 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ion crimin appeal 89 1961 appeal special leav ...</td>\n",
       "      <td>appel tri murder fact establish quarrel appel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>civil appeal 50 1961 appeal special leav award...</td>\n",
       "      <td>employ appel cross cutter saw mill ask show be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Judgement  \\\n",
       "0  appeal 101 1959 appeal special leav judgment o...   \n",
       "1  appeal 52 1957 appeal judgment decre date apri...   \n",
       "2  appeal no 45 46 1959 appeal special leav judgm...   \n",
       "3  ion crimin appeal 89 1961 appeal special leav ...   \n",
       "4  civil appeal 50 1961 appeal special leav award...   \n",
       "\n",
       "                                             Summary  \n",
       "0  appel displac person west pakistan grant quasi...  \n",
       "1  appel respond owner adjoin collieri suit prese...  \n",
       "2  respond firm claim exempt sale tax articl 286 ...  \n",
       "3  appel tri murder fact establish quarrel appel ...  \n",
       "4  employ appel cross cutter saw mill ask show be...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\transformers\\utils\\hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# downloading the RAG model and initialising\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = [\n",
    "    tokenizer(\n",
    "        judgement, return_tensors=\"pt\", padding=True, truncation=True\n",
    "    ).input_ids.squeeze(0)\n",
    "    for judgement in train_data[\"Judgement\"]\n",
    "]\n",
    "test_inputs = [\n",
    "    tokenizer(\n",
    "        judgement, return_tensors=\"pt\", padding=True, truncation=True\n",
    "    ).input_ids.squeeze(0)\n",
    "    for judgement in test_data[\"Judgement\"]\n",
    "]\n",
    "\n",
    "train_labels = [\n",
    "    tokenizer(\n",
    "        summary, return_tensors=\"pt\", padding=True, truncation=True\n",
    "    ).input_ids.squeeze(0)\n",
    "    for summary in train_data[\"Summary\"]\n",
    "]\n",
    "test_labels = [\n",
    "    tokenizer(\n",
    "        summary, return_tensors=\"pt\", padding=True, truncation=True\n",
    "    ).input_ids.squeeze(0)\n",
    "    for summary in test_data[\"Summary\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, labels = zip(*batch)\n",
    "\n",
    "    # Pad inputs and labels to the same length\n",
    "    inputs_padded = pad_sequence(\n",
    "        inputs, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    labels_padded = pad_sequence(\n",
    "        labels, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    # Ensure tensors are of type torch.long\n",
    "    return inputs_padded.long(), labels_padded.long()\n",
    "\n",
    "\n",
    "# Prepare Datasets\n",
    "train_dataset = TextDataset(train_inputs, train_labels)\n",
    "test_dataset = TextDataset(test_inputs, test_labels)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:496: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 14.77 GiB is allocated by PyTorch, and 84.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minputs, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1640\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1636\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1637\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1638\u001b[0m         )\n\u001b[1;32m-> 1640\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m   1641\u001b[0m     input_ids,\n\u001b[0;32m   1642\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1643\u001b[0m     decoder_input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[0;32m   1644\u001b[0m     encoder_outputs\u001b[38;5;241m=\u001b[39mencoder_outputs,\n\u001b[0;32m   1645\u001b[0m     decoder_attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask,\n\u001b[0;32m   1646\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1647\u001b[0m     decoder_head_mask\u001b[38;5;241m=\u001b[39mdecoder_head_mask,\n\u001b[0;32m   1648\u001b[0m     cross_attn_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_head_mask,\n\u001b[0;32m   1649\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1650\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1651\u001b[0m     decoder_inputs_embeds\u001b[38;5;241m=\u001b[39mdecoder_inputs_embeds,\n\u001b[0;32m   1652\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1653\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1654\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1655\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1656\u001b[0m )\n\u001b[0;32m   1658\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1659\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m lm_logits \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\u001b[38;5;241m.\u001b[39mto(lm_logits\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1526\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1519\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1520\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1521\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1522\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1523\u001b[0m     )\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1526\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[0;32m   1527\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[0;32m   1528\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask,\n\u001b[0;32m   1529\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1530\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1531\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mdecoder_head_mask,\n\u001b[0;32m   1532\u001b[0m     cross_attn_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_head_mask,\n\u001b[0;32m   1533\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1534\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39mdecoder_inputs_embeds,\n\u001b[0;32m   1535\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1536\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1537\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1538\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1539\u001b[0m )\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1379\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1366\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1367\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1368\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1376\u001b[0m         use_cache,\n\u001b[0;32m   1377\u001b[0m     )\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1379\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m   1380\u001b[0m         hidden_states,\n\u001b[0;32m   1381\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1382\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1383\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m   1384\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39m(head_mask[idx] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1385\u001b[0m         cross_attn_layer_head_mask\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m             cross_attn_head_mask[idx] \u001b[38;5;28;01mif\u001b[39;00m cross_attn_head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1387\u001b[0m         ),\n\u001b[0;32m   1388\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m   1389\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1390\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1391\u001b[0m     )\n\u001b[0;32m   1392\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:665\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    663\u001b[0m self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;66;03m# add present self-attn cache to positions 1,2 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m--> 665\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    666\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    667\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[0;32m    668\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    669\u001b[0m     layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[0;32m    670\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    671\u001b[0m )\n\u001b[0;32m    672\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    673\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:517\u001b[0m, in \u001b[0;36mBartSdpaAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;66;03m# Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;66;03m# partitioned across GPUs when using tensor-parallelism.\u001b[39;00m\n\u001b[0;32m    515\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[1;32m--> 517\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj(attn_output)\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m, past_key_value\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 14.77 GiB is allocated by PyTorch, and 84.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids=inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1) Dataset is downloaded from https://github.com/Law-AI/summarization from the link: https://zenodo.org/records/7152317#.Yz6mJ9JByC0\n",
    "IN-Abs data is used. I used a subset of train-data of 10 documents (.txt files) for training and 100 documents (.txt files) for testing\n",
    "I took a small subset for training since sufficient RAM wasn't available on my local device and Google Colab as well.\n",
    "\n",
    "2) Imported judgement and summary files from train-data and test-data and preprocessed it\n",
    "\n",
    "3) facebook/bart-large model is downloaded to work as the Generative component of this project\n",
    "from Hugging Face\n",
    "\n",
    "4) Input and output tensors are prepared for training and testing a sequence-to-sequence model\n",
    "by tokenizing text data.\n",
    "\n",
    "5) TextDataset class for handling tokenized input and output pairs, and uses a collate_fn function to\n",
    "pad sequences to uniform length within batches. It then creates DataLoader instances for training\n",
    "and testing, which handle batching and shuffling of the data. \n",
    "\n",
    "6) Model is trained for 3 epochs using GPU. However, training was not possible since sufficient RAM wasn't\n",
    "available on my local device and Google Colab as well.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
